{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fnKXkk4J9Q5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33u_Ekqp9rKo",
        "outputId": "fde83305-58b8-41b4-f6de-325e66b7ea71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, TimeDistributed, Conv2D, MaxPooling2D, Flatten, SimpleRNN, LSTM, Dropout, Dense, BatchNormalization\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "1qY57AAMY0Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the folder containing the good and bad exercise videos\n",
        "video_folder = '/content/drive/My Drive/APS360H1/data'  # Adjust this path as necessary\n",
        "\n",
        "img_height, img_width = 64, 64  # Dimensions for resizing frames\n",
        "sequence_length = 30  # Number of frames to consider in each sequence"
      ],
      "metadata": {
        "id": "VNniWR52Y5h2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames_from_video(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, (img_height, img_width))\n",
        "        frame = img_to_array(frame)\n",
        "        frames.append(frame)\n",
        "        frame_count += 1\n",
        "        if frame_count == sequence_length:\n",
        "            break\n",
        "    cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "def load_videos(video_folder):\n",
        "    X, y_class, y_assessment = [], [], []\n",
        "    assessment_classes = os.listdir(video_folder)\n",
        "\n",
        "    for assessment in assessment_classes:\n",
        "        assessment_folder = os.path.join(video_folder, assessment)\n",
        "        if not os.path.isdir(assessment_folder):  # Skip non-directory files\n",
        "            continue\n",
        "        class_names = os.listdir(assessment_folder)\n",
        "        class_indices = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
        "\n",
        "        for class_name in class_names:\n",
        "            class_folder = os.path.join(assessment_folder, class_name)\n",
        "            for video_name in os.listdir(class_folder):\n",
        "                video_path = os.path.join(class_folder, video_name)\n",
        "                if not video_path.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):  # Skip non-video files\n",
        "                    continue\n",
        "                frames = extract_frames_from_video(video_path)\n",
        "                if len(frames) == sequence_length:\n",
        "                    X.append(frames)\n",
        "                    y_class.append(class_indices[class_name])\n",
        "                    y_assessment.append(1 if assessment == 'good' else 0)  # 1 for good, 0 for bad\n",
        "\n",
        "    X = np.array(X)\n",
        "    y_class = np.array(y_class)\n",
        "    y_assessment = np.array(y_assessment)\n",
        "    if len(X) == 0 or len(y_class) == 0:\n",
        "        return None, None, None  # Return None if no valid data is found\n",
        "\n",
        "    y_class = to_categorical(y_class, num_classes=len(class_names))\n",
        "    y_assessment = np.array(y_assessment)\n",
        "    return X, y_class, y_assessment\n",
        "\n",
        "def CNN_LSTM(input_shape, num_classes):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # TimeDistributed CNN Layers\n",
        "    x = TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'))(input_layer)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
        "\n",
        "    x = TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same'))(x)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
        "\n",
        "    x = TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same'))(x)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
        "\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "\n",
        "    # LSTM Layer\n",
        "    x = LSTM(128, return_sequences=False)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Dense Layers for Exercise Classification\n",
        "    classification_output = Dense(128, activation='relu')(x)\n",
        "    classification_output = Dropout(0.5)(classification_output)\n",
        "    classification_output = Dense(num_classes, activation='softmax', name='classification_output')(classification_output)\n",
        "\n",
        "    # Dense Layers for Good/Bad Assessment\n",
        "    assessment_output = Dense(128, activation='relu')(x)\n",
        "    assessment_output = Dropout(0.5)(assessment_output)\n",
        "    assessment_output = Dense(1, activation='sigmoid', name='assessment_output')(assessment_output)\n",
        "\n",
        "    # Model with two outputs\n",
        "    model = Model(inputs=input_layer, outputs=[classification_output, assessment_output])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss={'classification_output': 'categorical_crossentropy', 'assessment_output': 'binary_crossentropy'},\n",
        "                  metrics={'classification_output': 'accuracy', 'assessment_output': 'accuracy'})\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "p56O_2ulhInW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_RNN_LSTM(input_shape, num_classes):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # TimeDistributed CNN Layers\n",
        "    x = TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'))(input_layer)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
        "\n",
        "    x = TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same'))(x)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
        "\n",
        "    x = TimeDistributed(Conv2D(128, (3, 3), activation='relu', padding='same'))(x)\n",
        "    x = TimeDistributed(BatchNormalization())(x)\n",
        "    x = TimeDistributed(MaxPooling2D((2, 2)))(x)\n",
        "\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "\n",
        "    # RNN Layer\n",
        "    x = SimpleRNN(64, return_sequences=True)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # LSTM Layer\n",
        "    x = LSTM(128, return_sequences=False)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Dense Layers for Exercise Classification\n",
        "    classification_output = Dense(128, activation='relu')(x)\n",
        "    classification_output = Dropout(0.5)(classification_output)\n",
        "    classification_output = Dense(num_classes, activation='softmax', name='classification_output')(classification_output)\n",
        "\n",
        "    # Dense Layers for Good/Bad Assessment\n",
        "    assessment_output = Dense(128, activation='relu')(x)\n",
        "    assessment_output = Dropout(0.5)(assessment_output)\n",
        "    assessment_output = Dense(1, activation='sigmoid', name='assessment_output')(assessment_output)\n",
        "\n",
        "    # Model with two outputs\n",
        "    model = Model(inputs=input_layer, outputs=[classification_output, assessment_output])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss={'classification_output': 'categorical_crossentropy', 'assessment_output': 'binary_crossentropy'},\n",
        "                  metrics={'classification_output': 'accuracy', 'assessment_output': 'accuracy'})\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "CJNeUQf-qh-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load videos\n",
        "X, y_class, y_assessment = load_videos(video_folder)"
      ],
      "metadata": {
        "id": "8d-dmj-9o-Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Check if data was loaded correctly\n",
        "if X is None or y_class is None or y_assessment is None:\n",
        "    raise ValueError(\"No valid video data found. Please check your video paths and formats.\")\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "X_train_val, X_test, y_class_train_val, y_class_test, y_assessment_train_val, y_assessment_test = train_test_split(\n",
        "    X, y_class, y_assessment, test_size=0.15, random_state=42)\n",
        "\n",
        "X_train, X_val, y_class_train, y_class_val, y_assessment_train, y_assessment_val = train_test_split(\n",
        "    X_train_val, y_class_train_val, y_assessment_train_val, test_size=0.1765, random_state=42)  # 0.1765 * 0.85 ≈ 0.15\n",
        "\n",
        "# Build and compile the model\n",
        "input_shape = (sequence_length, img_height, img_width, 3)\n",
        "num_classes = len(os.listdir(os.path.join(video_folder, 'good')))\n",
        "model = CNN_RNN_LSTM(input_shape, num_classes)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, {'classification_output': y_class_train, 'assessment_output': y_assessment_train},\n",
        "                    epochs=20, batch_size=8, validation_data=(X_val, {'classification_output': y_class_val, 'assessment_output': y_assessment_val}))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc_class, test_acc_assessment = model.evaluate(X_test, {'classification_output': y_class_test, 'assessment_output': y_assessment_test})\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Classification Accuracy: {test_acc_class}')\n",
        "print(f'Test Assessment Accuracy: {test_acc_assessment}')\n",
        "\n",
        "# Save the model\n",
        "model.save('/content/drive/My Drive/APS360H1/exercise_assessment_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62RWGKISY-VL",
        "outputId": "c82fbffc-6875-4ba6-85bd-9b666839d5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m737s\u001b[0m 13s/step - assessment_output_accuracy: 0.5289 - classification_output_accuracy: 0.2768 - loss: 2.2782 - val_assessment_output_accuracy: 0.6277 - val_classification_output_accuracy: 0.3404 - val_loss: 2.0204\n",
            "Epoch 2/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m745s\u001b[0m 13s/step - assessment_output_accuracy: 0.6112 - classification_output_accuracy: 0.3693 - loss: 2.0790 - val_assessment_output_accuracy: 0.5745 - val_classification_output_accuracy: 0.3617 - val_loss: 2.0516\n",
            "Epoch 3/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m703s\u001b[0m 12s/step - assessment_output_accuracy: 0.6083 - classification_output_accuracy: 0.3586 - loss: 2.1005 - val_assessment_output_accuracy: 0.5957 - val_classification_output_accuracy: 0.2872 - val_loss: 2.1006\n",
            "Epoch 4/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m685s\u001b[0m 12s/step - assessment_output_accuracy: 0.6218 - classification_output_accuracy: 0.3403 - loss: 2.1098 - val_assessment_output_accuracy: 0.5638 - val_classification_output_accuracy: 0.2872 - val_loss: 2.1786\n",
            "Epoch 5/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 12s/step - assessment_output_accuracy: 0.6269 - classification_output_accuracy: 0.3160 - loss: 2.0441 - val_assessment_output_accuracy: 0.6064 - val_classification_output_accuracy: 0.3830 - val_loss: 2.0199\n",
            "Epoch 6/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m688s\u001b[0m 12s/step - assessment_output_accuracy: 0.6226 - classification_output_accuracy: 0.3079 - loss: 2.0850 - val_assessment_output_accuracy: 0.5851 - val_classification_output_accuracy: 0.3723 - val_loss: 2.0935\n",
            "Epoch 7/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 12s/step - assessment_output_accuracy: 0.5923 - classification_output_accuracy: 0.3240 - loss: 2.0473 - val_assessment_output_accuracy: 0.5532 - val_classification_output_accuracy: 0.3936 - val_loss: 2.0884\n",
            "Epoch 8/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m672s\u001b[0m 12s/step - assessment_output_accuracy: 0.6421 - classification_output_accuracy: 0.3533 - loss: 2.0512 - val_assessment_output_accuracy: 0.5745 - val_classification_output_accuracy: 0.3617 - val_loss: 2.1308\n",
            "Epoch 9/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m666s\u001b[0m 12s/step - assessment_output_accuracy: 0.6576 - classification_output_accuracy: 0.3849 - loss: 1.9908 - val_assessment_output_accuracy: 0.5638 - val_classification_output_accuracy: 0.3511 - val_loss: 2.0609\n",
            "Epoch 10/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m707s\u001b[0m 12s/step - assessment_output_accuracy: 0.6339 - classification_output_accuracy: 0.4123 - loss: 1.9617 - val_assessment_output_accuracy: 0.5532 - val_classification_output_accuracy: 0.4468 - val_loss: 2.0080\n",
            "Epoch 11/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m713s\u001b[0m 13s/step - assessment_output_accuracy: 0.6861 - classification_output_accuracy: 0.3952 - loss: 1.8867 - val_assessment_output_accuracy: 0.5851 - val_classification_output_accuracy: 0.4255 - val_loss: 2.0067\n",
            "Epoch 12/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m739s\u001b[0m 13s/step - assessment_output_accuracy: 0.6810 - classification_output_accuracy: 0.4079 - loss: 1.8680 - val_assessment_output_accuracy: 0.6383 - val_classification_output_accuracy: 0.4362 - val_loss: 1.9322\n",
            "Epoch 13/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m740s\u001b[0m 13s/step - assessment_output_accuracy: 0.6379 - classification_output_accuracy: 0.4504 - loss: 1.8399 - val_assessment_output_accuracy: 0.6277 - val_classification_output_accuracy: 0.3723 - val_loss: 2.0098\n",
            "Epoch 14/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m707s\u001b[0m 13s/step - assessment_output_accuracy: 0.5796 - classification_output_accuracy: 0.4751 - loss: 1.8775 - val_assessment_output_accuracy: 0.4894 - val_classification_output_accuracy: 0.4468 - val_loss: 2.1228\n",
            "Epoch 15/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m732s\u001b[0m 13s/step - assessment_output_accuracy: 0.7193 - classification_output_accuracy: 0.4832 - loss: 1.7725 - val_assessment_output_accuracy: 0.6596 - val_classification_output_accuracy: 0.4043 - val_loss: 2.0069\n",
            "Epoch 16/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m754s\u001b[0m 13s/step - assessment_output_accuracy: 0.6609 - classification_output_accuracy: 0.4950 - loss: 1.8426 - val_assessment_output_accuracy: 0.6383 - val_classification_output_accuracy: 0.4681 - val_loss: 2.0189\n",
            "Epoch 17/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m721s\u001b[0m 13s/step - assessment_output_accuracy: 0.7445 - classification_output_accuracy: 0.5024 - loss: 1.6493 - val_assessment_output_accuracy: 0.6489 - val_classification_output_accuracy: 0.5319 - val_loss: 1.8627\n",
            "Epoch 18/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m707s\u001b[0m 13s/step - assessment_output_accuracy: 0.7572 - classification_output_accuracy: 0.5182 - loss: 1.6669 - val_assessment_output_accuracy: 0.5957 - val_classification_output_accuracy: 0.4362 - val_loss: 2.1907\n",
            "Epoch 19/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m730s\u001b[0m 13s/step - assessment_output_accuracy: 0.7064 - classification_output_accuracy: 0.5093 - loss: 1.6418 - val_assessment_output_accuracy: 0.6596 - val_classification_output_accuracy: 0.4574 - val_loss: 2.1018\n",
            "Epoch 20/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m743s\u001b[0m 13s/step - assessment_output_accuracy: 0.7684 - classification_output_accuracy: 0.5172 - loss: 1.6086 - val_assessment_output_accuracy: 0.6489 - val_classification_output_accuracy: 0.4255 - val_loss: 2.1167\n",
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3s/step - assessment_output_accuracy: 0.7065 - classification_output_accuracy: 0.5220 - loss: 1.8278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.8955925703048706\n",
            "Test Classification Accuracy: 0.7021276354789734\n",
            "Test Assessment Accuracy: 0.4893617033958435\n"
          ]
        }
      ]
    }
  ]
}